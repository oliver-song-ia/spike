# Jetson éƒ¨ç½²æŒ‡å—

## ğŸ“¦ éœ€è¦çš„æ–‡ä»¶

### ä»ç”µè„‘å¤åˆ¶åˆ° Jetson:

```bash
# 1. ONNX æ¨¡å‹æ–‡ä»¶
experiments/Custom/pretrained-full/log/best_model.onnx  (249 MB)

# 2. Python è„šæœ¬
tensorrt_wrapper.py              # TensorRT æ¨ç†å°è£…
build_jetson_engine.py          # ONNX â†’ TensorRT è½¬æ¢è„šæœ¬
jetson_simple_inference.py      # ç®€å•æ¨ç†ç¤ºä¾‹
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹æ³• 1: è‡ªåŠ¨éƒ¨ç½² (æ¨è)

```bash
# ä¿®æ”¹è„šæœ¬ä¸­çš„ Jetson IP åœ°å€
nano deploy_to_jetson.sh

# è¿è¡Œéƒ¨ç½²è„šæœ¬
./deploy_to_jetson.sh
```

### æ–¹æ³• 2: æ‰‹åŠ¨éƒ¨ç½²

#### æ­¥éª¤ 1: å¤åˆ¶æ–‡ä»¶åˆ° Jetson

```bash
# åˆ›å»ºç›®å½•
ssh jetson@<IP> "mkdir -p ~/spike_inference"

# å¤åˆ¶æ–‡ä»¶
scp experiments/Custom/pretrained-full/log/best_model.onnx \
    jetson@<IP>:~/spike_inference/

scp tensorrt_wrapper.py \
    build_jetson_engine.py \
    jetson_simple_inference.py \
    jetson@<IP>:~/spike_inference/
```

#### æ­¥éª¤ 2: SSH ç™»å½•åˆ° Jetson

```bash
ssh jetson@<IP>
cd ~/spike_inference
```

#### æ­¥éª¤ 3: æ„å»º TensorRT å¼•æ“

```bash
# å®‰è£…ä¾èµ– (å¦‚æœè¿˜æ²¡æœ‰)
pip3 install numpy pycuda

# è½¬æ¢ ONNX åˆ° TensorRT (è€—æ—¶ 5-15 åˆ†é’Ÿ)
python3 build_jetson_engine.py \
    --onnx best_model.onnx \
    --output best_model_jetson.engine \
    --fp16
```

#### æ­¥éª¤ 4: æµ‹è¯•æ¨ç†

```bash
python3 jetson_simple_inference.py
```

## ğŸ’» åœ¨ä½ çš„ä»£ç ä¸­ä½¿ç”¨

```python
from jetson_simple_inference import JetsonPoseEstimator

# åˆå§‹åŒ– (åªéœ€ä¸€æ¬¡)
estimator = JetsonPoseEstimator("best_model_jetson.engine")

# å®æ—¶æ¨ç†å¾ªç¯
while True:
    # è·å–ç‚¹äº‘æ•°æ® [3, 4096, 3] æˆ– [1, 3, 4096, 3]
    point_cloud = get_point_cloud_from_sensor()

    # æ¨ç† (~2-5ms on Jetson)
    joints = estimator.estimate_pose(point_cloud)

    # joints shape: [15, 3]
    # 15 ä¸ªå…³èŠ‚, æ¯ä¸ª 3D åæ ‡ (x, y, z)

    # ä½¿ç”¨ç»“æœ
    head_pos = joints[0]  # å¤´éƒ¨ä½ç½®
    print(f"Head: {head_pos}")
```

## ğŸ“Š é¢„æœŸæ€§èƒ½

| Jetson å‹å· | æ¨ç†æ—¶é—´ | FPS |
|-------------|---------|-----|
| Jetson Orin | ~2-3 ms | 300-400 |
| Jetson Xavier | ~4-6 ms | 150-250 |
| Jetson Nano | ~15-25 ms | 40-60 |

## ğŸ”§ è¿›ä¸€æ­¥ä¼˜åŒ– (å¯é€‰)

### INT8 é‡åŒ– (å¯å†å¿« 2-4å€)

éœ€è¦å‡†å¤‡æ ¡å‡†æ•°æ®é›†,ä¿®æ”¹ `build_jetson_engine.py` æ·»åŠ  INT8 æ”¯æŒã€‚

### DLA åŠ é€Ÿå™¨ (éƒ¨åˆ† Jetson æ”¯æŒ)

åœ¨æ„å»ºå¼•æ“æ—¶ä½¿ç”¨ DLA å¯é™ä½åŠŸè€—ã€‚

## ğŸ“ æ–‡ä»¶è¯´æ˜

| æ–‡ä»¶ | è¯´æ˜ |
|------|------|
| `best_model.onnx` | ONNX æ¨¡å‹ (é€šç”¨æ ¼å¼) |
| `best_model_jetson.engine` | Jetson ä¸“ç”¨ TensorRT å¼•æ“ |
| `tensorrt_wrapper.py` | TensorRT æ¨ç†å°è£…ç±» |
| `build_jetson_engine.py` | ONNX â†’ TensorRT è½¬æ¢å·¥å…· |
| `jetson_simple_inference.py` | ç®€å•æ˜“ç”¨çš„æ¨ç†æ¥å£ |
| `deploy_to_jetson.sh` | è‡ªåŠ¨éƒ¨ç½²è„šæœ¬ |

## âš ï¸ é‡è¦æç¤º

1. **ä¸è¦**ç›´æ¥å¤åˆ¶ç”µè„‘ä¸Šçš„ `.engine` æ–‡ä»¶åˆ° Jetson
   - TensorRT å¼•æ“æ˜¯ç¡¬ä»¶ç›¸å…³çš„
   - å¿…é¡»åœ¨ Jetson ä¸Šé‡æ–°æ„å»º

2. **åªéœ€è¦** `.onnx` æ–‡ä»¶æ˜¯è·¨å¹³å°çš„
   - åœ¨ Jetson ä¸Šç”¨ ONNX æ„å»ºä¸“ç”¨å¼•æ“

3. **å†…å­˜å ç”¨**
   - TensorRT å¼•æ“: ~126 MB
   - æ¨ç†æ—¶æ˜¾å­˜: ~200-300 MB

## ğŸ†˜ æ•…éšœæ’é™¤

### å¦‚æœæ„å»ºå¼•æ“å¤±è´¥:

```bash
# æ£€æŸ¥ TensorRT æ˜¯å¦å®‰è£…
python3 -c "import tensorrt; print(tensorrt.__version__)"

# æ£€æŸ¥ CUDA
python3 -c "import pycuda.driver; print('CUDA OK')"

# é™ä½ workspace å¤§å°
python3 build_jetson_engine.py --onnx best_model.onnx \
    --output best_model_jetson.engine --workspace 1
```

### å¦‚æœæ¨ç†å¤±è´¥:

```bash
# æ£€æŸ¥å¼•æ“æ–‡ä»¶
ls -lh best_model_jetson.engine

# é‡æ–°æ„å»ºå¼•æ“
rm best_model_jetson.engine
python3 build_jetson_engine.py --onnx best_model.onnx \
    --output best_model_jetson.engine
```

## ğŸ“ è”ç³»

å¦‚æœ‰é—®é¢˜ï¼Œæ£€æŸ¥ `README_TensorRT.md` äº†è§£æ›´å¤šæŠ€æœ¯ç»†èŠ‚ã€‚
